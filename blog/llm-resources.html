<!DOCTYPE html>
<html style="font-size: 16px;" lang="en">

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <meta name="keywords" content="Forrest Bicker">
  <meta name="description" content="Projects">
  <title>LLM Resources</title>
  <link rel="stylesheet" href="../nicepage.css" media="screen">
  <link rel="stylesheet" href="../Post-Template.css" media="screen">
  <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
  <script class="u-script" type="text/javascript" src="../nicepage.js" defer=""></script>

  <link rel="icon" href="../assets/favicon.ico">



  <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
  <link id="u-page-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
  <link id="u-header-footer-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
  <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "Forrest Bicker",
		"logo": "assets/favicon.ico",
		"sameAs": [
				"https://www.linkedin.com/in/forrestbicker",
				"https://forrestbicker.com/resume"
		]
}</script>
  <meta name="theme-color" content="#478ac9">
  <meta name="google-site-verification" content="bpfgu58ztjB8bb9YQQ1kkUGBZ3xB4DMkIaf-6FH6d_g">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BF80ELN7MR"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-BF80ELN7MR');
  </script>
  <link rel="stylesheet" href="https://forrestbicker.com/fonts/OperatorMono/OperatorMono.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
  <link href="https://fonts.googleapis.com/css2?family=Alex+Brush&amp;display=swap" rel="stylesheet">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
  <link href="https://fonts.googleapis.com/css2?family=Alex+Brush&amp;family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&amp;display=swap" rel="stylesheet">

  <meta data-intl-tel-input-cdn-path="intlTelInput/">
</head>

<body data-path-to-root="./" data-include-products="false" class="u-body u-xl-mode" data-lang="en">
  <header class="u-clearfix u-custom-color-10 u-header u-sticky u-sticky-3292 u-header" id="sec-929d" data-animation-name="" data-animation-duration="0" data-animation-delay="0" data-animation-direction="">
    <div class="u-clearfix u-sheet u-sheet-1">
      <a href="../" data-page-id="2011114386" class="u-image u-logo u-image-1" data-image-width="64" data-image-height="64" title="Forrest Bicker">
        <img src="../assets/favicon.ico" class="u-logo-image u-logo-image-1">
      </a>
      <nav class="u-menu u-menu-one-level u-offcanvas u-menu-1" role="navigation" aria-label="Menu navigation">
        <div class="menu-collapse" style="font-size: 1.25rem; letter-spacing: 0px; font-weight: 800;">
          <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-text-active-color u-custom-text-color u-custom-text-hover-color u-custom-top-bottom-menu-spacing u-hamburger-link u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#" tabindex="-1" aria-label="Open menu" aria-controls="c881">
            <svg class="u-svg-link" viewBox="0 0 24 24">
              <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use>
            </svg>
            <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg">
              <g>
                <rect y="1" width="16" height="2"></rect>
                <rect y="7" width="16" height="2"></rect>
                <rect y="13" width="16" height="2"></rect>
              </g>
            </svg>
          </a>
        </div>
        <div class="u-custom-menu u-nav-container">
          <ul class="u-nav u-unstyled u-nav-1" role="menubar">
            <li role="none" class="u-nav-item"><a tabindex="-1" role="menuitem" class="u-button-style u-nav-link u-text-active-custom-color-12 u-text-custom-color-6 u-text-hover-palette-2-dark-1" href="https://www.linkedin.com/in/forrestbicker/" target="_blank" aria-haspopup="true" style="padding: 10px 20px;">Resume</a>
            </li>
            <li role="none" class="u-nav-item"><a role="menuitem" class="u-button-style u-nav-link u-text-active-custom-color-12 u-text-custom-color-6 u-text-hover-palette-2-dark-1" href="../blog" style="padding: 10px 20px;">Projects</a>
            </li>
          </ul>
        </div>
        <div class="u-custom-menu u-nav-container-collapse" id="c881" role="region" aria-label="Menu panel">
          <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
            <div class="u-inner-container-layout u-sidenav-overflow">
              <div class="u-menu-close" tabindex="-1" aria-label="Close menu"></div>
              <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2" role="menubar">
                <li role="none" class="u-nav-item"><a tabindex="-1" role="menuitem" class="u-button-style u-nav-link" href="https://www.linkedin.com/in/forrestbicker/" target="_blank" aria-haspopup="true">Resume</a>
                </li>
                <li role="none" class="u-nav-item"><a role="menuitem" class="u-button-style u-nav-link" href="../blog">Projects</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
        </div>
      </nav>
      <p class="operator-mono u-custom-font u-font-open-sans u-text u-text-custom-color-10 u-text-1">
        <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-custom-color-6 u-btn-1" data-href="../" data-page-id="2011114386" href="../">Forrest Bicker</a>
      </p>
    </div>
    <style class="u-sticky-style" data-style-id="3292">
      .u-sticky-fixed.u-sticky-3292,
      .u-body.u-sticky-fixed .u-sticky-3292 {
        box-shadow: 5px 5px 20px 0 rgba(0, 0, 0, 0.4) !important
      }
    </style>
  </header>
  <section class="u-align-center u-clearfix u-custom-color-6 u-section-1" id="sec-2271">
    <div class="u-clearfix u-sheet u-sheet-1"><!--post_details--><!--post_details_options_json--><!--{"source":""}--><!--/post_details_options_json--><!--blog_post-->
      <div class="u-container-style u-expanded-width u-post-details u-post-details-1">
        <div class="u-container-layout u-container-layout-1">
          <div class="blog-title u-container-style u-expanded-width-xs u-group u-shape-rectangle u-group-1-wide">
            <div class="u-container-layout u-container-layout-2">
              <div class="u-border-4 u-border-black u-expanded-width u-line u-line-horizontal u-line-1"></div><!--blog_post_header-->
              <h2 class="operator-mono u-blog-control u-custom-font u-font-open-sans u-text u-text-1">LLM Resources</h2><!--/blog_post_header-->
              <div class="u-border-4 u-border-black u-expanded-width u-line u-line-horizontal u-line-2"></div><!--blog_post_content-->
              <div class="u-align-justify u-blog-control u-post-content u-text u-text-2 fr-view">
                <p>A lot of people haved asked how they can learn more about modern ML and LLMs, so I thought I'd compile an (incomplete) selection of papers for getting started.&nbsp;</p>
                <p>1. Antebellum</p>
                <ul>
                  <li style="font-size: 0.875rem;">NNLM (Bengio 2003, UMontréal) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">A Neural Probabilistic Language Model</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Linguistic Regularities (Mikolov 2013, Microsoft Research) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://aclanthology.org/N13-1090/" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Linguistic Regularities in Continuous Space Word Representations</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Word2Vec (Mikolov 2013, Microsoft Research) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1310.4546" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Distributed Representations of Words and Phrases and their Compositionality</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GloVe (Pennington 2014, Stanford NLP) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://aclanthology.org/D14-1162/" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">GloVe: Global Vectors for Word Representation</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">ELMo (Peters 2018, UWash) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1802.05365" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Deep Contextualized Word Representations</a>&nbsp;</li>
                </ul>
                <p>2. Early Transformers</p>
                <ul>
                  <li style="font-size: 0.875rem;">Transformers (Vaswani 2017, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1706.03762" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Attention Is All You Need</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GPT1 (Radford 2018, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Improving Language Understanding by Generative Pre-Training</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">BERT (Devlin 2018, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1810.04805" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GPT2 (Radford 2019, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Language Models are Unsupervised Multitask Learners</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">RoBERTa (Liu 2019, FAIR) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1907.11692" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">ALBERT (Lan 2019, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1909.11942" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">ALBERT: A Lite BERT for Self-supervised Learning of Language Representation</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">DistilBERT (Sanh 2019, Hugging Face) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1910.01108" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Lottery hypothesis (Frankle 2018, MIT) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1803.03635" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">| Magnitude Pruning (Han 2015, Stanford & NVIDIA) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/pdf/1506.02626" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Learning both Weights and Connections for Efficient Neural Networks</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">| Sparse models (Gupta 2017, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1710.01878" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">To prune, or not to prune: exploring the efficacy of pruning for model compression</a>&nbsp;</li>
                </ul>
                <p>3. Scaling</p>
                <ul>
                  <li style="font-size: 0.875rem;">T5 (Raffel 2019, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1910.10683" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Double descent (Nakkiran 2019, Harvard & OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1912.02292" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Deep Double Descent: Where Bigger Models and More Data Hurt</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Scaling Laws (Kaplan 2020, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2001.08361" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Scaling Laws for Neural Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GPT-3 (Brown 2020, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2005.14165" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Language Models are Few-Shot Learners</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">| Locally Banded Sparse Attention (Child 2019, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1904.10509" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Generating Long Sequences with Sparse Transformers</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">| Gradient Noise Scale (McCandlish 2018, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1812.06162" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">An Empirical Model of Large-Batch Training</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Rotary Encodings (Su 2021, Zhuiyi) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2104.09864" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Chinchilla (Hoffmann 2022, DeepMind) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2203.15556" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Training Compute-Optimal Large Language Model</a>&nbsp;</li>
                  <br>
                  <li style="font-size: 0.875rem;">MoE (Shazeer 2017, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1701.06538" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts Layer</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GShard (Lepikhin 2020, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2006.16668" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Switch Transformer (Fedus 2021, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2101.03961" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>&nbsp;</li>
                  <br>
                  <sup>Graveyard:</sup>
                  <li style="font-size: 0.875rem;">Transformer-XL (Dai 2019, CMU & Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1901.02860" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Longformer (Beltagy 2020, AI2) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2004.05150" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Longformer: The Long-Document Transformer</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Reformer (Kitaev 2020, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2001.04451" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Reformer: The Efficient Transformer</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">BigBird (Zaheer 2020, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2007.14062" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Big Bird: Transformers for Longer Sequences</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Performer (Choromanski 2021, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2009.14794" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Rethinking Attention with Performers</a>&nbsp;</li>
                </ul>
                <p>4. Alignment and Agentic</p>
                <ul>
                  <li style="font-size: 0.875rem;">RLHF (Ouyang 2022, OpenAI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2203.02155" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Training language models to follow instructions with human feedback</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">DPO (Rafailov 2024, Stanford) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2305.18290" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GRPO (Shao 2024, DeepSeek) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2402.03300" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Scratchpad (Nye 2021, MIT & Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2112.00114" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Chain-of-Thought (Wei 2023, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2201.11903" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Toolformer (Schick 2023, Meta AI) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2302.04761" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Toolformer: Language Models Can Teach Themselves to Use Tools</a>&nbsp;</li>
                </ul>
                <p>5. ML Systems</p>
                <ul>
                  <li style="font-size: 0.875rem;">Flash Attention (Dao 2022, Stanford) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2205.14135" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">| <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">CUDA Programming Guide</a> (NVIDIA)&nbsp;</li>
                  <li style="font-size: 0.875rem;">| <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Deep Learning Performance</a> (NVIDIA)&nbsp;</li>
                  <li style="font-size: 0.875rem;">| <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Making Deep Learning Go Brrrr From First Principles</a> (He 2022)&nbsp;</li>
                  <li style="font-size: 0.875rem;">Flash Attention 2 (Dao 2023, Stanford) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2307.08691" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Flash Attention 3 (Shah 2024, Stanford) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2407.08608" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a>&nbsp;</li>
                  <br>
                  <sup>Quantization&nbsp;</sup>
                  <li style="font-size: 0.875rem;">BFloat16 (Wang 2019, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">BFloat16: The secret to high performance on Cloud TPUs</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Microscaling Formats (Rouhani 2023, Microsoft & AMD & Intel & Meta & NVIDIA & Qualcomm) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2310.10537" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Microscaling Data Formats for Deep Learning</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">LLM.int8() (Dettmers 2022, UWash & Meta & HuggingFace) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2208.07339" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">SmoothQuant (Xiao 2022, MIT & NVIDIA) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2211.10438" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GPTQ (Frantar 2022, ETH Zurich) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2210.17323" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">LLM-QAT (Liu 2023, Meta) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2305.17888" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">LLM-QAT: Data-Free Quantization Aware Training for Large Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">AWQ (Lin 2023, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2402.09154" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Attacking Large Language Models with Projected Gradient Descent</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">QLoRA (Dettmers 2023, UWash) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2305.14314" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">QLoRA: Efficient Finetuning of Quantized LLMs</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">BitNet (Wang 2023, Microsoft) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2310.11453" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">BitNet: Scaling 1-bit Transformers for Large Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">1.58 BitNet (Ma 2024, Microsoft) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2402.17764" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Quantization Scaling Laws (Dettmers 2022, UWash) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2212.09720" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">The case for 4-bit precision: k-bit Inference Scaling Laws</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">FlexGen (Sheng 2023, Stanford et al.) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2303.06865" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">KIVI (Liu 2024, CMU) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2402.02750" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</a>&nbsp;</li>

                  <br>
                  <li style="font-size: 0.875rem;">DeepSpeed (Rasley 2020, Microsoft) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1910.02054" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">ZeRO optimizer; trillion-scale feasibility</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">NVLAMB Optimizer (Hsieh 2020, NVIDIA) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1904.00962" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Large-batch training stabilizer</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Mesh-TensorFlow (Ginsburg 2020, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/1811.02084" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Multi-axis distributed tensor computation</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">TPU v3 Scaling (Jouppi 2020, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2104.05765" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Hardware-level optimizations for LLM throughput</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">FSDP / FairScale (Baines 2021, FAIR) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2101.06840" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Sharded data-parallel training</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">LoRA (Hu 2021, Microsoft) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2106.09685" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">LoRA: Low-Rank Adaptation of Large Language Models</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">GQA (Ainslie 2023, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2305.13245" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">KV Cache (Pope 2022, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2211.05102" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Efficiently Scaling Transformer Inference</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">vLLM / Paged Attention (Kwon 2023, Google) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2309.06180" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Efficient Memory Management for Large Language Model Serving with PagedAttention</a>&nbsp;</li>
                  <li style="font-size: 0.875rem;">Muon (Liu 2025, Moonshot AI & UCLA) - <a class="u-active-none u-border-none u-btn u-button-link u-button-style u-hover-none u-none u-text-palette-1-base u-block-ba78-5" href="https://arxiv.org/abs/2502.16982" rel="noopener noreferrer" target="_blank" style="background-image: none; padding-top: 0; padding-bottom: 0; padding-left: 0; padding-right: 0">Muon Is Scalable For Llm Training</a>&nbsp;</li>
                </ul>
              </div><!--/blog_post_content-->
            </div>
          </div>
        </div>
      </div><!--/blog_post--><!--/post_details-->
    </div>
  </section>



  <footer class="feeter u-align-center u-clearfix u-container-align-center u-custom-color-10 u-footer u-footer" id="sec-90f7">
    <div class="u-clearfix u-sheet u-sheet-1">
      <div class="custom-expanded data-layout-selected u-clearfix u-layout-custom-sm u-layout-custom-xs u-layout-wrap u-layout-wrap-1">
        <div class="u-layout">
          <div class="u-layout-row">
            <div class="u-container-style u-layout-cell u-size-30 u-layout-cell-1">
              <div class="u-container-layout u-valign-middle u-container-layout-1">
                <div class="u-social-icons u-social-icons-1">
                  <a class="u-social-url" target="_blank" data-type="LinkedIn" title="LinkedIn" href="https://www.linkedin.com/in/forrestbicker"><span class="u-icon u-social-icon u-social-linkedin u-text-custom-color-7 u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style="">
                        <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-5b01"></use>
                      </svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-5b01">
                        <circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle>
                        <path fill="#FFFFFF" d="M41.3,83.7H27.9V43.4h13.4V83.7z M34.6,37.9L34.6,37.9c-4.6,0-7.5-3.1-7.5-7c0-4,3-7,7.6-7s7.4,3,7.5,7
      C42.2,34.8,39.2,37.9,34.6,37.9z M89.6,83.7H76.2V62.2c0-5.4-1.9-9.1-6.8-9.1c-3.7,0-5.9,2.5-6.9,4.9c-0.4,0.9-0.4,2.1-0.4,3.3v22.5
      H48.7c0,0,0.2-36.5,0-40.3h13.4v5.7c1.8-2.7,5-6.7,12.1-6.7c8.8,0,15.4,5.8,15.4,18.1V83.7z"></path>
                      </svg></span>
                  </a>
                  <a class="u-social-url" target="_blank" data-type="Custom" title="Resume" href="https://forrestbicker.com/resume"><span class="u-file-icon u-icon u-social-custom u-social-icon u-text-custom-color-7 u-icon-2"><img src="../images/resume.png" alt=""></span>
                  </a>
                </div>
              </div>
            </div>
            <div class="u-container-style u-layout-cell u-size-30 u-layout-cell-2">
              <div class="u-container-layout u-valign-middle-md u-valign-middle-sm u-valign-middle-xs u-container-layout-2">
                <p class="quote u-align-right u-small-text u-text u-text-variant u-text-1">A world​ to win.<br>
                </p>
                <p class="u-align-right u-small-text u-text u-text-variant u-text-2"><span style="font-style: normal;">Forrest&nbsp;</span><span style="font-style: normal;">평화&nbsp;</span><span style="font-style: normal;">Bicker </span>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>